{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # dealing with arrays\n",
    "import re, string  # regular expressions and string objects\n",
    "import codecs      # file I/O\n",
    "import time        # timing code blocks\n",
    "import os          # various OS interfaces\n",
    "import sys\n",
    "from bs4 import BeautifulSoup   # XML parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task description\n",
    "\n",
    "The main tasks for this project can be summarised as:\n",
    "\n",
    "+ Parsing and cleaning up TMX files\n",
    "+ Splitting source/target sentences into aligned monolingual files \n",
    "+ Computing summary metrics on each monolingual file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parsing TMX files and splitting by language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first – loading TMX files. Rather than writing our own parsing functions from scratch using regular expressions, a more robust (and faster) way is to use a pre-existing tool. A popular Python library for parsing HTML and XML documents (like TMX files) is BeautifulSoup (BS4).\n",
    "\n",
    "BeautifulSoup often does a good job of figuring out which encoding to use from header tags, so we'll leave it for now on this first pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data_from_tmx(filepath):\n",
    "    \"\"\"\n",
    "    Load TMX file, parse XML, build a nested soup object.\n",
    "    \"\"\"\n",
    "    print('Extracting data from \"{0}\" ({1:.2f}MB)... this might take 1-2 mins...'.format(filepath, os.path.getsize(filepath)/1000000))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # the b mode specifier treats files as binaries for now\n",
    "    with open(filepath, 'rb') as file:\n",
    "          soup = BeautifulSoup(file, \"lxml\")\n",
    "    print('Data extracted in {0:.2f} seconds.'.format(time.time() - start_time))\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this files looks like when read in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from \"small_sample.tmx\" (0.01MB)... this might take 1-2 mins...\n",
      "Data extracted in 0.01 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" ?><html><body><tmx version=\"1.4\">\n",
       "<header adminlang=\"en\" creationdate=\"Sun Oct 23 00:52:07 2011\" creationtool=\"Uplug\" creationtoolversion=\"unknown\" datatype=\"PlainText\" o-tmf=\"unknown\" segtype=\"sentence\" srclang=\"en\"></header>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>EMEA/ H/ C/ 471</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>EMEA/ H/ C/ 471</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>ABILIFY</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>ABILIFY</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>What is Abilify?</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Qu’ est -ce qu'Abilify?</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>Abilify is a medicine containing the active substance aripiprazole.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Abilify est un médicament qui contient le principe actif aripiprazole.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>It is available as 5 mg, 10 mg, 15 mg and 30 mg tablets, as 10 mg, 15 mg and 30 mg orodispersible tablets (tablets that dissolve in the mouth), as an oral solution (1 mg/ ml) and as a solution for injection (7.5 mg/ ml).</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Il est disponible sous la forme de comprimés de 5 mg, 10 mg, 15 mg et 30 mg, de comprimés orodispersibles (comprimés qui se dissolvent dans la bouche) de 10 mg, 15 mg et 30 mg, sous la forme d’ une solution buvable (1 mg/ ml) et sous la forme d’ une solution injectable (7,5 mg/ ml).</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>What is Abilify used for?</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Dans quel cas Abilify est -il utilisé?</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>Abilify is used to treat adults with the following mental illnesses: • schizophrenia, a mental illness with a number of symptoms, including disorganised thinking and speech, hallucinations (hearing or seeing things that are not there), suspiciousness and delusions (mistaken beliefs); • bipolar I disorder, a mental illness in which patients have manic episodes (periods of abnormally high mood), alternating with periods of normal mood.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Abilify est utilisé pour traiter les personnes adultes souffrant des maladies mentales suivantes: • schizophrénie, une maladie mentale qui se manifeste par tout un éventail de symptômes, comme la difficulté à organiser ses pensées et son discours, les hallucinations (le fait d’ entendre ou de voir des choses qui ne sont pas présentes), la méfiance et les délires (perception erronée); • les troubles bipolaires de type I, maladie mentale dans laquelle les patients connaissent des épisodes maniaques (période d’ humeur anormalement élevée), en alternance avec des épisodes d’ humeur normale.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>They may also have episodes of depression.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Ils peuvent également traverser des épisodes de dépression.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>Abilify is used to treat moderate to severe manic episodes and to prevent manic episodes in patients who have responded to the medicine in the past.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Abilify est utilisé pour traiter les épisodes maniaques modérés à sévères et pour prévenir les épisodes maniaques chez les patients ayant répondu au médicament antérieurement.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>The solution for injection is used for the rapid control of agitation or disturbed behaviour when taking the medicine by mouth is not appropriate.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>La solution injectable est utilisée en vue de la maîtrise rapide de l’ agitation ou de comportements perturbés lorsque la prise du médicament par voie orale n’ est pas appropriée.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>How is Abilify used?</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Comment Abilify est -il utilisé?</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>For schizophrenia, the recommended starting dose is 10 or 15 mg by mouth per day.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Pour la schizophrénie, la dose de départ recommandée d’ Abilify par voie orale est de 10 à 15 mg par jour.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>The maintenance dose is 15 mg once a day, but higher doses may benefit some patients.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>La dose d’ entretien est de 15 mg une fois par jour, mais des doses plus élevées peuvent s'avérer bénéfiques chez certains patients.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>For bipolar disorder, the recommended starting dose is 15 mg by mouth once a day, either on its own or in combination with other medicines.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Pour les troubles bipolaires, la dose de départ recommandée est de 15 mg par voie orale une fois par jour, soit seul, soit en combinaison avec d’ autres médicaments.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>Some patients may benefit from a higher dose.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Une dose plus élevée peut s’ avérer bénéfique chez certains patients.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>To prevent manic episodes, the same dose should be continued.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Pour prévenir les épisodes maniaques, la posologie doit être maintenue à l’ identique.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>For both illnesses, the oral solution or orodispersible tablets can be used in patients who have difficulty swallowing tablets.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Pour les deux maladies, la solution orale et les comprimés orodispersibles peuvent être utilisés chez les patients qui ont des difficultés à avaler des comprimés.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>The orodispersible tablets are taken by being placed on the tongue, where they disintegrate quickly in the saliva, or by mixing them in water before swallowing.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Les comprimés orodispersibles sont soit posés sur la langue, et se dissolvent rapidement dans la salive, ou dissous dans de l’ eau avant ingestion.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>Abilify can be taken with or without food.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Abilify peut être pris accompagné d’ aliments ou non.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>The daily dose of Abilify should not exceed 30 mg, but this dose should be used with caution in patients who have severe problems with their liver.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>La dose quotidienne d’ Abilify ne doit pas dépasser 30 mg, mais cette dose sera utilisée avec précaution chez les personnes souffrant de problèmes hépatiques (du foie) sévères.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>The dose of Abilify should be adjusted in patients who are taking other medicines that are broken down in the same way as Abilify.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>La dose d’ Abilify sera ajustée chez les patients qui prennent d’ autres médicaments qui se décomposent dans l’ organisme de la même manière qu’ Abilify.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>Abilify has not been studied in children aged below 18 years or adults aged over 65 years.</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>Abilify n’ a pas été étudié chez les enfants de moins de 18 ans ni chez les adultes âgés de plus de 65 ans.</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>258 LT-48452 Kaunas +370 37 323 144</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>258 LT-48452 Kaunas +370 37 323 144</seg></tuv>\n",
       "</tu>\n",
       "<tu>\n",
       "<tuv xml:lang=\"en\"><seg>22</seg></tuv>\n",
       "<tuv xml:lang=\"fr\"><seg>25</seg></tuv>\n",
       "</tu>\n",
       "</tmx>\n",
       "</body></html>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = extract_data_from_tmx('small_sample.tmx')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the things we want are on lines with `<tuv>` tags. We can find all of them like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tuv xml:lang=\"en\"><seg>EMEA/ H/ C/ 471</seg></tuv>,\n",
       " <tuv xml:lang=\"fr\"><seg>EMEA/ H/ C/ 471</seg></tuv>,\n",
       " <tuv xml:lang=\"en\"><seg>ABILIFY</seg></tuv>,\n",
       " <tuv xml:lang=\"fr\"><seg>ABILIFY</seg></tuv>,\n",
       " <tuv xml:lang=\"en\"><seg>What is Abilify?</seg></tuv>,\n",
       " <tuv xml:lang=\"fr\"><seg>Qu’ est -ce qu'Abilify?</seg></tuv>]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuv_lines = soup.find_all('tuv')\n",
    "tuv_lines[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a line of these, we can grab the language and sentence tags from this soup object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tuv xml:lang=\"en\"><seg>Abilify has not been studied in children aged below 18 years or adults aged over 65 years.</seg></tuv>\n"
     ]
    }
   ],
   "source": [
    "one_tuv_line = tuv_lines[42]\n",
    "print(one_tuv_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "Abilify has not been studied in children aged below 18 years or adults aged over 65 years.\n"
     ]
    }
   ],
   "source": [
    "print(one_tuv_line.attrs['xml:lang'])\n",
    "print(one_tuv_line.find('seg').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build up arrays of paired language/sentence tags for a corpus, which looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['en', 'EMEA/ H/ C/ 471'],\n",
       "       ['fr', 'EMEA/ H/ C/ 471'],\n",
       "       ['en', 'ABILIFY'],\n",
       "       ['fr', 'ABILIFY'],\n",
       "       ['en', 'What is Abilify?'],\n",
       "       ['fr', \"Qu’ est -ce qu'Abilify?\"]],\n",
       "      dtype='<U593')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuv_line_sentences = np.array([[line.attrs['xml:lang'], line.find('seg').get_text()] for line in tuv_lines])\n",
    "tuv_line_sentences[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to split the TMX file into one file per language, we need to find the total number of languages present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en', 'fr']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(tuv_line_sentences[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then filter to get just the, say, English sentences using simple array slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EMEA/ H/ C/ 471', 'ABILIFY', 'What is Abilify?',\n",
       "       'Abilify is a medicine containing the active substance aripiprazole.',\n",
       "       'It is available as 5 mg, 10 mg, 15 mg and 30 mg tablets, as 10 mg, 15 mg and 30 mg orodispersible tablets (tablets that dissolve in the mouth), as an oral solution (1 mg/ ml) and as a solution for injection (7.5 mg/ ml).',\n",
       "       'What is Abilify used for?',\n",
       "       'Abilify is used to treat adults with the following mental illnesses: • schizophrenia, a mental illness with a number of symptoms, including disorganised thinking and speech, hallucinations (hearing or seeing things that are not there), suspiciousness and delusions (mistaken beliefs); • bipolar I disorder, a mental illness in which patients have manic episodes (periods of abnormally high mood), alternating with periods of normal mood.',\n",
       "       'They may also have episodes of depression.',\n",
       "       'Abilify is used to treat moderate to severe manic episodes and to prevent manic episodes in patients who have responded to the medicine in the past.',\n",
       "       'The solution for injection is used for the rapid control of agitation or disturbed behaviour when taking the medicine by mouth is not appropriate.'],\n",
       "      dtype='<U593')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuv_line_sentences[tuv_line_sentences[:,0] == 'en'][:,1][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all of these things together, we can write the below function. \n",
    "\n",
    "[Note: This code used to work by assuming sentences in each language were in a consistent ordering throughout the file (e.g. ['en', 'fr', 'es', 'en', 'fr', 'es'...], but I realised this isn't actually an enforced standard of any sort, and now it extracts monolingual sentences by explicitly searching for the tags]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_into_separate_files(doc_soup, encoding='utf8', monolingual_path='split_monolingual_files'):\n",
    "    \"\"\"\n",
    "    Takes soup object, detects languages, extracts sentences, \n",
    "    outputs separate text file per language. \n",
    "    \"\"\"\n",
    "    \n",
    "    # look for <tuv> tags to find sentence text in the doc soup\n",
    "    print('Finding and organising language tags and sentences in the TMX file...')\n",
    "    sentence_lines = doc_soup.find_all('tuv')\n",
    "    \n",
    "    # get array of language tag and sentence text arrays\n",
    "    try:\n",
    "        langs_sentences = np.array([[line.attrs['xml:lang'], line.find('seg').get_text()] for line in sentence_lines])\n",
    "    except KeyError:\n",
    "        langs_sentences = np.array([[line.attrs['lang'], line.find('seg').get_text()] for line in sentence_lines])\n",
    "\n",
    "    # find all langs present\n",
    "    languages = list(set(langs_sentences[:,0]))\n",
    "    print('Detected {0} languages in file: {1}'.format(len(languages), languages))\n",
    "    \n",
    "    # make dir to store split files if it doesn't already exist\n",
    "    if not os.path.exists(monolingual_path):\n",
    "        os.makedirs(monolingual_path)\n",
    "    \n",
    "    # iterate over all languages in the file\n",
    "    for lang in languages:\n",
    "\n",
    "        # get all sentences in a given language and\n",
    "        # put each sentence on a new line\n",
    "        sentences_in_given_lang = langs_sentences[langs_sentences[:,0] == lang][:,1]\n",
    "        text = '\\n'.join('{}'.format(sentence) for sentence in sentences_in_given_lang)\n",
    "\n",
    "        # write out files with sentences\n",
    "        out_file = os.path.join(monolingual_path, lang + '_sentences.txt')\n",
    "        print('Writing split {0} sentences to {1}...'.format(lang, out_file))\n",
    "        with codecs.open(out_file,'w',encoding=encoding) as file:\n",
    "            file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this toy case, the output of the function would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding and organising language tags and sentences in the TMX file...\n",
      "Detected 2 languages in file: ['en', 'fr']\n",
      "Writing split en sentences to split_monolingual_files/en_sentences.txt...\n",
      "Writing split fr sentences to split_monolingual_files/fr_sentences.txt...\n"
     ]
    }
   ],
   "source": [
    "split_into_separate_files(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing each monolingual file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading monolingual files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load up each file that we produced. Here is a function to load a single file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_monolingual_corpus(file):\n",
    "    \"\"\"\n",
    "    Loads a monolingual corpus containing \n",
    "    one sentence per line in UTF-8 format.\n",
    "    \"\"\"\n",
    "    print('-'*50)\n",
    "    print('Working on file \"{0}\" of size {1:.2f}MB.'.format(file, os.path.getsize(file)/1000000))\n",
    "    try:\n",
    "        with codecs.open(file,'rb',encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with codecs.open(file,'rb',encoding='utf16') as f:\n",
    "            text = f.read()\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the function is trying to load using UTF-8 and switching to UTF-16 if that fails (there are probably smarter ways to guess encoding). For example, the `fr_sentence.txt` file can be loaded like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Working on file \"split_monolingual_files/fr_sentences.txt\" of size 0.00MB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"EMEA/ H/ C/ 471\\nABILIFY\\nQu’ est -ce qu'Abilify?\\nAbilify est un médicament qui contient le principe actif aripiprazole.\\nIl est disponible sous la forme de comprimés de 5 mg, 10 mg, 15 mg et 30 mg, de c\""
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = load_monolingual_corpus(os.path.join('split_monolingual_files', 'fr_sentences.txt'))\n",
    "text[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising white space and splitting corpus by sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function to normalise the white space in a given corpus. We can compile a regex function to search for white space characters, while keeping new line `\\n` and carriage return `\\r` characters out of the regex so as not to mess up the sentence splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def whitespace_normalise_corpus(text):\n",
    "    \"\"\"\n",
    "    Reduce whitespace in a corpus down to a single space; preserve new lines. \n",
    "    \"\"\"\n",
    "    print('Normalising white space in corpus...')\n",
    "    pattern = re.compile(r\"[ \\t\\f\\v]+\")\n",
    "    clean = pattern.sub(\" \", text) \n",
    "    return(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalising white space in corpus...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"EMEA/ H/ C/ 471\\nABILIFY\\nQu’ est -ce qu'Abilify?\\nAbilify est un médicament qui contient le principe actif aripiprazole.\\nIl est disponible sous la forme de comprimés de 5 mg, 10 mg, 15 mg et 30 mg, de c\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = whitespace_normalise_corpus(text)\n",
    "text[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't see the difference in this example, but for instance if we had the input sentence `'  this is    a test \\t\\t sentence \\n   and another  sentence  \\n ok   one more   '`, the function would reduce white spaces down to 1 space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalising white space in corpus...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' this is a test sentence \\n and another sentence \\n ok one more . '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = ' this is    a test \\t\\t sentence \\n   and another  sentence  \\n ok   one more .  '\n",
    "whitespace_normalise_corpus(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could extend this whitespace normalisation to delete any whitespaces at the start or end of new lines using the `string.strip()` method. First let's split the corpus into sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_corpus_into_sentences(text):\n",
    "    \"\"\"\n",
    "    Split a corpus into sentences; returns array of\n",
    "    one array per sentence.\n",
    "    \"\"\"\n",
    "    print('Finding sentences in the corpus...')\n",
    "    sentences = text.split('\\n')\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding sentences in the corpus...\n"
     ]
    }
   ],
   "source": [
    "sentence_arrays = split_corpus_into_sentences(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMEA/ H/ C/ 471',\n",
       " 'ABILIFY',\n",
       " \"Qu’ est -ce qu'Abilify?\",\n",
       " 'Abilify est un médicament qui contient le principe actif aripiprazole.',\n",
       " 'Il est disponible sous la forme de comprimés de 5 mg, 10 mg, 15 mg et 30 mg, de comprimés orodispersibles (comprimés qui se dissolvent dans la bouche) de 10 mg, 15 mg et 30 mg, sous la forme d’ une solution buvable (1 mg/ ml) et sous la forme d’ une solution injectable (7,5 mg/ ml).']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_arrays[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. And running this on our test sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a quick function to get rid of any whitespaces lingering at the beggining and end of lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_whitespace_padding(sentence_arrays):\n",
    "    \"\"\"\n",
    "    Remove leading and trailing white spaces in sentences.\n",
    "    \"\"\"\n",
    "    print('Removing any whitespace padding around sentences...')\n",
    "    sentence_arrays = [sentence.strip() for sentence in sentence_arrays]\n",
    "    return(sentence_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing any whitespace padding around sentences...\n"
     ]
    }
   ],
   "source": [
    "sentence_arrays = remove_whitespace_padding(sentence_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenising sentence arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an array containing a sentence, we can split by white space to create tokens. Note that this function only works on languages that use spaces to denote token boundaries (e.g. excludes Chinese, Japanese, Thai, Khmer etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sentence_arrays(sentence_arrays):\n",
    "    \"\"\"\n",
    "    Splitting sentence arrays on spaces. \n",
    "    \"\"\"\n",
    "    print('Tokenising sentence arrays...')\n",
    "    sentence_tokens = [sentence.split() for sentence in sentence_arrays]\n",
    "    return(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can tokenise our corpus and see what the results look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenising sentence arrays...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['EMEA/', 'H/', 'C/', '471'],\n",
       " ['ABILIFY'],\n",
       " ['Qu’', 'est', '-ce', \"qu'Abilify?\"],\n",
       " ['Abilify',\n",
       "  'est',\n",
       "  'un',\n",
       "  'médicament',\n",
       "  'qui',\n",
       "  'contient',\n",
       "  'le',\n",
       "  'principe',\n",
       "  'actif',\n",
       "  'aripiprazole.']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_sentences = tokenize_sentence_arrays(sentence_arrays)\n",
    "tokenised_sentences[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other cleaning functions of potential interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions like these could be integrated later as optional preprocessing steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardise_case(tokens):\n",
    "    \"\"\"\n",
    "    Convert all tokens to lower case. \n",
    "    \"\"\"\n",
    "    print('Standardising case...')\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(tokens):\n",
    "    \"\"\"\n",
    "    Removing punctuation.\n",
    "    \"\"\"\n",
    "    print('Removing punctuation...')\n",
    "    pattern = re.compile('[\\W_]+', re.UNICODE)\n",
    "    tokens = [pattern.sub('', token) for token in tokens]\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting together all the monolingual corpus processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous preprocessing functions can be united into one pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_monolingual_corpus(text, whitespace_normalise=True, whitespace_padding_normalise=True, \n",
    "                               standardise_case=False, remove_punctuation=False):\n",
    "    \"\"\"\n",
    "    Cleans a loaded corpus using a variety of options. \n",
    "    Returns cleaned arrays of individual tokens. \n",
    "    \"\"\"\n",
    "    \n",
    "    if whitespace_normalise:\n",
    "        text = whitespace_normalise_corpus(text)\n",
    "    \n",
    "    sentence_arrays = split_corpus_into_sentences(text)\n",
    "    \n",
    "    if whitespace_padding_normalise:\n",
    "        sentence_arrays = remove_whitespace_padding(sentence_arrays)\n",
    "    \n",
    "    tokenised_sentences = tokenize_sentence_arrays(sentence_arrays)\n",
    "        \n",
    "    return(sentence_arrays, tokenised_sentences)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should do the job of cleaning up a monolingual to make it ready for summarisation. Trying this out on the tiny example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Working on file \"split_monolingual_files/fr_sentences.txt\" of size 0.00MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n"
     ]
    }
   ],
   "source": [
    "text = load_monolingual_corpus(os.path.join('split_monolingual_files', 'fr_sentences.txt'))\n",
    "sentence_arrays, tokenised_sentences = process_monolingual_corpus(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Computing summary statistics on corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task asked to calculate some simple summary statistics on each corpus. This is very easy given some sentence arrays and tokenised sentences, which were already created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_summary_report(sentence_arrays, tokenised_sentences):\n",
    "    \"\"\"\n",
    "    Compute simple summary statistics on corpus. \n",
    "    \"\"\"\n",
    "    print('Generating report...')\n",
    "    flat_tokens = [token for sentence in tokenised_sentences for token in sentence]\n",
    "\n",
    "    print('Number of sentences in this corpus: {}'.format(len(sentence_arrays)))\n",
    "    print('Number of unique sentences in this corpus: {}'.format(len(set(sentence_arrays))))\n",
    "    print('Number of total tokens: ' + str(len(flat_tokens)))\n",
    "    print('Number of unique tokens: ' + str(len(set(flat_tokens))) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report...\n",
      "Number of sentences in this corpus: 24\n",
      "Number of unique sentences in this corpus: 24\n",
      "Number of total tokens: 482\n",
      "Number of unique tokens: 236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "write_summary_report(sentence_arrays, tokenised_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** The number of unique sentences/tokens will heavily depend on preprocessing choices (case standardisation, stemming, lemitisation, removing non-alphanumeric characters, removing punctuation etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing every file in the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be good to be able to apply this processing to every file in the directory generated to hold the monolingual aligned files. This can be done just by iterating over files in the directory and processing each file in turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_monolingual_files(directory='split_monolingual_files'):\n",
    "    \"\"\"\n",
    "    Apply the processing pipeline on all files in a directory.  \n",
    "    \"\"\"\n",
    "    for file in os.listdir(directory):\n",
    "        if not file.startswith('.') and os.path.getsize(directory+file)>0:\n",
    "            text = load_monolingual_corpus(os.path.join(directory, file))\n",
    "            sentence_arrays, tokenised_sentences = process_monolingual_corpus(text)\n",
    "            write_summary_report(sentence_arrays, tokenised_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Working on file \"split_monolingual_files/en_sentences.txt\" of size 0.00MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 24\n",
      "Number of unique sentences in this corpus: 24\n",
      "Number of total tokens: 396\n",
      "Number of unique tokens: 199\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"split_monolingual_files/fr_sentences.txt\" of size 0.00MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 24\n",
      "Number of unique sentences in this corpus: 24\n",
      "Number of total tokens: 482\n",
      "Number of unique tokens: 236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_monolingual_files(os.path.join('split_monolingual_files', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Example output: Running the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the above functions run on raw TMX files all together. \n",
    "\n",
    "I've noticed that TMX files can have quite a few different formats, so I've tried to be general and test on some different data sources, but I'm sure there's more work to be done here to handle edge cases and improve generalisation. \n",
    "\n",
    "The following TMX files were used:\n",
    "\n",
    "1. Your example file, from the European Medicines Agency (EMEA) parallel [corpus](http://opus.nlpl.eu/EMEA.php) containing French/English sentences\n",
    "2. The Bulgarian-Hungarian corpus (48.3MB) from the Europarl [dataset](http://opus.nlpl.eu/Europarl.php). It appears that something strange has happened with this file, maybe the curators have mislabelled the languages. I am just taking their data as granted and not attempting to fix it. \n",
    "3. An 11-language [dataset](https://data.europa.eu/euodp/data/dataset/dgt-translation-memory) from the Acquis Communautaire, the body of European legislation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Running the code on the EMEA corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from \"en-fr.tmx\" (115.13MB)... this might take 1-2 mins...\n",
      "Data extracted in 74.35 seconds.\n",
      "Detected 2 languages in file: ['en', 'fr']\n",
      "Writing split en sentences to en-fr_split_monolingual_files/en_sentences.txt...\n",
      "Writing split fr sentences to en-fr_split_monolingual_files/fr_sentences.txt...\n",
      "--------------------------------------------------\n",
      "Working on file \"en-fr_split_monolingual_files/en_sentences.txt\" of size 34.54MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 373152\n",
      "Number of unique sentences in this corpus: 290498\n",
      "Number of total tokens: 5361232\n",
      "Number of unique tokens: 133845\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"en-fr_split_monolingual_files/fr_sentences.txt\" of size 41.74MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 373152\n",
      "Number of unique sentences in this corpus: 313052\n",
      "Number of total tokens: 6222687\n",
      "Number of unique tokens: 154231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmx_file = 'en-fr.tmx'\n",
    "filestring, _ = os.path.splitext(tmx_file)\n",
    "\n",
    "doc_soup = extract_data_from_tmx(tmx_file)\n",
    "split_into_separate_files(doc_soup, encoding='utf8', monolingual_path=filestring+'_split_monolingual_files')\n",
    "process_monolingual_files(os.path.join(filestring+'_split_monolingual_files', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running the code on the Europarl Bulgarian-Hungarian corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from \"bg-hu.tmx\" (207.61MB)... this might take 1-2 mins...\n",
      "Data extracted in 112.63 seconds.\n",
      "Finding and organising language tags and sentences in the TMX file...\n",
      "Detected 2 languages in file: ['hu', 'bg']\n",
      "Writing split hu sentences to bg-hu_split_monolingual_files/hu_sentences.txt...\n",
      "Writing split bg sentences to bg-hu_split_monolingual_files/bg_sentences.txt...\n",
      "--------------------------------------------------\n",
      "Working on file \"bg-hu_split_monolingual_files/bg_sentences.txt\" of size 104.97MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 370236\n",
      "Number of unique sentences in this corpus: 368042\n",
      "Number of total tokens: 8675621\n",
      "Number of unique tokens: 187145\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"bg-hu_split_monolingual_files/hu_sentences.txt\" of size 64.51MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 370236\n",
      "Number of unique sentences in this corpus: 368140\n",
      "Number of total tokens: 7573447\n",
      "Number of unique tokens: 381441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmx_file = 'bg-hu.tmx'\n",
    "filestring, _ = os.path.splitext(tmx_file)\n",
    "\n",
    "doc_soup = extract_data_from_tmx(tmx_file)\n",
    "split_into_separate_files(doc_soup, encoding='utf8', monolingual_path=filestring+'_split_monolingual_files')\n",
    "process_monolingual_files(os.path.join(filestring+'_split_monolingual_files', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running the code on the 11-language corpus from the Acquis Communautaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from \"DGT-TM.tmx\" (0.29MB)... this might take 1-2 mins...\n",
      "Data extracted in 0.13 seconds.\n",
      "Finding and organising language tags and sentences in the TMX file...\n",
      "Detected 11 languages in file: ['EL-01', 'DE-DE', 'ES-ES', 'PT-PT', 'NL-NL', 'DA-01', 'FR-FR', 'FI-01', 'IT-IT', 'SV-SE', 'EN-GB']\n",
      "Writing split EL-01 sentences to DGT-TM_split_monolingual_filesEL-01_sentences.txt...\n",
      "Writing split DE-DE sentences to DGT-TM_split_monolingual_filesDE-DE_sentences.txt...\n",
      "Writing split ES-ES sentences to DGT-TM_split_monolingual_filesES-ES_sentences.txt...\n",
      "Writing split PT-PT sentences to DGT-TM_split_monolingual_filesPT-PT_sentences.txt...\n",
      "Writing split NL-NL sentences to DGT-TM_split_monolingual_filesNL-NL_sentences.txt...\n",
      "Writing split DA-01 sentences to DGT-TM_split_monolingual_filesDA-01_sentences.txt...\n",
      "Writing split FR-FR sentences to DGT-TM_split_monolingual_filesFR-FR_sentences.txt...\n",
      "Writing split FI-01 sentences to DGT-TM_split_monolingual_filesFI-01_sentences.txt...\n",
      "Writing split IT-IT sentences to DGT-TM_split_monolingual_filesIT-IT_sentences.txt...\n",
      "Writing split SV-SE sentences to DGT-TM_split_monolingual_filesSV-SE_sentences.txt...\n",
      "Writing split EN-GB sentences to DGT-TM_split_monolingual_filesEN-GB_sentences.txt...\n",
      "--------------------------------------------------\n",
      "Working on file \"DGT-TM_split_monolingual_files/bg_sentences.txt\" of size 0.06MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 416\n",
      "Number of unique sentences in this corpus: 403\n",
      "Number of total tokens: 8111\n",
      "Number of unique tokens: 2594\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"DGT-TM_split_monolingual_files/DA-01_sentences.txt\" of size 0.02MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 76\n",
      "Number of unique sentences in this corpus: 75\n",
      "Number of total tokens: 1308\n",
      "Number of unique tokens: 438\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"DGT-TM_split_monolingual_files/DE-DE_sentences.txt\" of size 0.02MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 76\n",
      "Number of unique sentences in this corpus: 75\n",
      "Number of total tokens: 1274\n",
      "Number of unique tokens: 402\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"DGT-TM_split_monolingual_files/EL-01_sentences.txt\" of size 0.02MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 76\n",
      "Number of unique sentences in this corpus: 75\n",
      "Number of total tokens: 1561\n",
      "Number of unique tokens: 439\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"DGT-TM_split_monolingual_files/EN-GB_sentences.txt\" of size 0.02MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 79\n",
      "Number of unique sentences in this corpus: 79\n",
      "Number of total tokens: 1579\n",
      "Number of unique tokens: 398\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"DGT-TM_split_monolingual_files/ES-ES_sentences.txt\" of size 0.02MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 76\n",
      "Number of unique sentences in this corpus: 75\n",
      "Number of total tokens: 1823\n",
      "Number of unique tokens: 425\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"DGT-TM_split_monolingual_files/FI-01_sentences.txt\" of size 0.02MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 76\n",
      "Number of unique sentences in this corpus: 74\n",
      "Number of total tokens: 1050\n",
      "Number of unique tokens: 430\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"DGT-TM_split_monolingual_files/FR-FR_sentences.txt\" of size 0.02MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 70\n",
      "Number of unique sentences in this corpus: 69\n",
      "Number of total tokens: 1542\n",
      "Number of unique tokens: 441\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"DGT-TM_split_monolingual_files/hu_sentences.txt\" of size 0.06MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 416\n",
      "Number of unique sentences in this corpus: 405\n",
      "Number of total tokens: 7882\n",
      "Number of unique tokens: 2565\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"DGT-TM_split_monolingual_files/IT-IT_sentences.txt\" of size 0.02MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 75\n",
      "Number of unique sentences in this corpus: 74\n",
      "Number of total tokens: 1406\n",
      "Number of unique tokens: 451\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"DGT-TM_split_monolingual_files/NL-NL_sentences.txt\" of size 0.02MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 76\n",
      "Number of unique sentences in this corpus: 75\n",
      "Number of total tokens: 1526\n",
      "Number of unique tokens: 394\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"DGT-TM_split_monolingual_files/PT-PT_sentences.txt\" of size 0.02MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 76\n",
      "Number of unique sentences in this corpus: 76\n",
      "Number of total tokens: 1602\n",
      "Number of unique tokens: 443\n",
      "\n",
      "--------------------------------------------------\n",
      "Working on file \"DGT-TM_split_monolingual_files/SV-SE_sentences.txt\" of size 0.02MB.\n",
      "Normalising white space in corpus...\n",
      "Finding sentences in the corpus...\n",
      "Removing any whitespace padding around sentences...\n",
      "Tokenising sentence arrays...\n",
      "Generating report...\n",
      "Number of sentences in this corpus: 76\n",
      "Number of unique sentences in this corpus: 76\n",
      "Number of total tokens: 1322\n",
      "Number of unique tokens: 417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmx_file = 'DGT-TM.tmx'\n",
    "filestring, _ = os.path.splitext(tmx_file)\n",
    "\n",
    "doc_soup = extract_data_from_tmx(tmx_file)\n",
    "split_into_separate_files(doc_soup, encoding='utf16', monolingual_path=filestring+'_split_monolingual_files')\n",
    "process_monolingual_files(os.path.join(filestring+'_split_monolingual_files', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packaging the above code into a stand-alone programme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standalone Python script is basically just a copy and paste of the above functions, with a few additions:\n",
    "\n",
    "+ simple argument parsing from command line input and checking for input validity (see below)\n",
    "+ writing a main function to execute the processing script if the script is run alone (otherwise, the functions can just imported for other purposes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's all sorts of wrong things that a user could provide as an argument. We can check a few quick ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_tmx_file_CLI(file):\n",
    "    \"\"\"\n",
    "    Checking if the file provided as a CLI argument\n",
    "    is ok to proceed with.\n",
    "    \"\"\"\n",
    "    \n",
    "    # check user has provided a file as input\n",
    "    assert len(sys.argv)==2, \\\n",
    "        print(\"Please provide the name of one TMX file to process.\")\n",
    "\n",
    "    # check file actually exists\n",
    "    assert os.path.exists(file), \\\n",
    "        print(\"A file with that name cannot be found.\")\n",
    "\n",
    "    # check that file is nonempty\n",
    "    assert os.path.getsize(file)>0, \\\n",
    "        print(\"This file doesn't contain anything.\")\n",
    "\n",
    "    # check if file extension is .tmx\n",
    "    filestring, extension = os.path.splitext(file)\n",
    "    assert extension=='.tmx', \\\n",
    "        print(\"Unsuitable file. File extension must be .tmx\")\n",
    "\n",
    "    return(filestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Technical details\n",
    "\n",
    "The code was originally developed using Python 3.5.2 on macOS Sierra 10.12. \n",
    "\n",
    "### Running on a Mac\n",
    "\n",
    "Create a virtual environment with:\n",
    "\n",
    "`virtualenv tmx_env`\n",
    "\n",
    "Activate the virtualenv:\n",
    "\n",
    "`source tmx_env/bin/activate`\n",
    "\n",
    "Install package requirements:\n",
    "\n",
    "`python3 install -r requirements.txt`\n",
    "\n",
    "Run code:\n",
    "\n",
    "`python parse_tmx.py small_sample.tmx`\n",
    "\n",
    "Note, sometimes lmxl installation wonky and you get a `bs4.FeatureNotFound: Couldn't find a tree builder` error. In this case `pip uninstall lmxl` then `python3 -m pip install lmxl` fixes it. \n",
    "\n",
    "When done, you can deactivate the env:\n",
    "\n",
    "`deactivate`\n",
    "\n",
    "\n",
    "### Running on a Windows machine\n",
    "\n",
    "Sorry about that, I didn't have time for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Future work\n",
    "\n",
    "This was intended to be a quick piece of work, but with a bit more time, I would work on:\n",
    "+ Ensuring Windows compatibility (mostly checking file pathing with os.path and new line/carriage return characters)\n",
    "+ Testing on more .tmx files to ensure generalisability\n",
    "+ Looking into speed optimisations\n",
    "+ Examining more edge cases\n",
    "+ Potential conversion to OOP, esp. if this is the paradigm in the group, rather than this more procedural approach. Could conceptualiase of a corpus and sentence being two classes with associated preprocessing methods\n",
    "+ Dockerising for easier transportability\n",
    "+ Support for automatic encoding detection on monolingual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
